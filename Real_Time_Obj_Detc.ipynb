{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Object Detection for Blind People"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to develop an assistive technology solution that enhances the independence and safety of visually impaired individuals. By integrating advanced computer vision techniques, this system will provide real-time feedback about the user's surroundings, helping them navigate and interact with their environment more effectively. The process is divided into 4 steps which are as follows :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download and import dependencies\n",
    "\n",
    "Download all the dependencies imported below and run the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPTImageProcessor, DPTForDepthEstimation, DetrImageProcessor, DetrForObjectDetection\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialise the models\n",
    "\n",
    "Run the below cell if you do not have a lot of computation power you can visit the hugginface (https://huggingface.co/models) website and try small models from the website you just need to replace the name of models specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large\")\n",
    "depth_model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
    "objd_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "objd_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write the Logic\n",
    "\n",
    "The below cell shows the logic for the predictions. The function below works in the following way:\n",
    "1. Get all the objects from the image.\n",
    "2. Get depth analysis of the image.\n",
    "3. Check only the depth analysis of the objects from the image.\n",
    "4. Make a beep sound if the object is close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_logic(image):\n",
    "    depth_inputs = depth_processor(images=image, return_tensors=\"pt\")\n",
    "    objd_inputs = objd_processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        depth_outputs = depth_model(**depth_inputs)\n",
    "        predicted_depth = depth_outputs.predicted_depth\n",
    "\n",
    "    objd_outputs = objd_model(**objd_inputs)\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.shape[:2],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    objd_target = torch.tensor([image.shape[:2]])\n",
    "    results = objd_processor.post_process_object_detection(objd_outputs, target_sizes=objd_target, threshold=0.9)[0]\n",
    "    output = prediction.squeeze().cpu().numpy()\n",
    "    formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "    for _, box in zip(results[\"labels\"], results[\"boxes\"]):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        row1, row2, column1, column2 = box[1], box[3], box[0], box[2]\n",
    "        cv2.rectangle(formatted, (int(column1), int(row1)), (int(column2), int(row2)), color=(0,0,255), thickness=1)\n",
    "        new_image = formatted[int(row1):int(row2), int(column1):int(column2)]\n",
    "        if new_image.max() >= 200:\n",
    "            frequency = 2500\n",
    "            duration = 1000\n",
    "            winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Start the camera\n",
    "\n",
    "Run the below cell to turn your camera on and read the images for predictions. Now point your camera tothe environment and listen to the beep sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(0)\n",
    "  \n",
    "while True: \n",
    "\n",
    "    ret, frame = vid.read()\n",
    "    model_logic(frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  \n",
    "        break  \n",
    "\n",
    "vid.release() \n",
    "cv2.destroyAllWindows() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
